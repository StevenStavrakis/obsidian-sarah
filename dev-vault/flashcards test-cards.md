<Card id="1" type="cloze">
[[Goal]]-oriented decisions are made on the assessment of expected reward or the value of an action
</Card>

<Card id="2" type="back-front">
<Front>What is an action-outcome decision?</Front>
<Back>When an action is made based on the evaluation of expected outcomes</Back>
</Card>

<Card id="3" type="back-front">
<Front>If you think there will be a positive outcome as a result of an action, then you will do that action to acquire the outcome. What is this an example of?</Front>
<Back>An action-outcome decision</Back>
</Card>

<Card id="4" type="back-front">
<Front>What is a stimulus-response decision?</Front>
<Back>A response is made based on the presence of a stimulus</Back>
</Card>

<Card id="5" type="cloze">
[[The]] ability of an outcome to reinforce a behavior depends on that outcome's value
</Card>

<Card id="6" type="back-front">
<Front>What are the components of value?</Front>
<Back>- Magnitude/payoff
- Probability
- Effort/cost</Back>
</Card>

<Card id="7" type="back-front">
<Front>What is "delay discounting"?</Front>
<Back>When the same reward magnitude has a lower value at longer delays</Back>
</Card>

<Card id="8" type="back-front">
<Front>You choose to take five hundred dollar tomorrow as opposed to eight hundred dollars next week. What is this an example of?</Front>
<Back>Delay discounting</Back>
</Card>

<Card id="9" type="back-front">
<Front>What did Hare et al. study (broadly)?</Front>
<Back>How value and control interact</Back>
</Card>

<Card id="10" type="back-front">
<Front>Who are the participant in Hare et al.?</Front>
<Back>Self-reported dieters</Back>
</Card>

<Card id="11" type="back-front">
<Front>What did self-reported dieters do during the rating phase in Hare et al?</Front>
<Back>Rated 50 different food items for taste and health on a five point scale</Back>
</Card>

<Card id="12" type="back-front">
<Front>What did self-reported dieters do during the decision phase in Hare et al?</Front>
<Back>They chose between different foods</Back>
</Card>

<Card id="13" type="back-front">
<Front>What are the independent variables in Hare et al?</Front>
<Back>1. Goal values (taste, health)
2. Whether participants were self-controllers or non-self-controllers (based on their choices)</Back>
</Card>

<Card id="14" type="back-front">
<Front>What is the "reference item" in Hare et al?</Front>
<Back>It is a participant specific item determined during the rating phase. An item that the participant has indicated as neutral for both taste and health</Back>
</Card>

<Card id="15" type="back-front">
<Front>How was a participant identified as a self-controller?</Front>
<Back>If the participant reliably chose the reference item over an unhealthy, tasty item, they were considered a self-controller</Back>
</Card>

<Card id="16" type="back-front">
<Front>How was a participant identified as a non-self-controller?</Front>
<Back>If they reliably chose an unhealthy, tasty item over the reference item</Back>
</Card>

<Card id="17" type="back-front">
<Front>Is there a brain region that is sensitive to value?</Front>
<Back>Yes. The Orbitofrontal Cortex OFC</Back>
</Card>

<Card id="18" type="back-front">
<Front>What evidence is there that the OFC is sensitive to value?</Front>
<Back>Hare et al. showed that OFC activity was higher for foods that were ranked higher in taste preference</Back>
</Card>

<Card id="19" type="cloze">
[[OFC]] activity does change as a function of taste preference. There is a positive, linear relationship between the self-reported tastiness of a food and OFC activity
</Card>

<Card id="20" type="back-front">
<Front>Does the value sensitivity to taste vs. health differ between self-controllers and non-self-controllers?</Front>
<Back>Yes. While OFC sensitivity is similar for high taste rating between both groups, non-self-controllers have dramatically lower OFC activity when presented high health items compared to self-controllers</Back>
</Card>

<Card id="21" type="cloze">
[[In]] non-self-controllers, OFC activity doesn't seem to scale with health rating at all.
</Card>

<Card id="22" type="cloze">
[[The]] more healthy something is, the more the OFC is active ONLY for self-controllers
</Card>

<Card id="23" type="back-front">
<Front>Is there a control-sensitive region with greater activity when control is exerted?</Front>
<Back>Yes. LPFC activity is greater when participants exert self-control and chose the reference item over the tasty item for both self-controllers and non-self-controllers</Back>
</Card>

<Card id="24" type="back-front">
<Front>How does LPFC activity differ between self-controllers and non-self-controllers in trials where the unhealthy item was chosen over the reference item?</Front>
<Back>Self-controllers showed effectively no OFC activity during failed trials, while non-self-controllers showed relatively significant OFC activity</Back>
</Card>

<Card id="25" type="back-front">
<Front>How does LPFC activity differ between self-controllers and non-self-controllers in trials where the healthy item was chosen over the reference item (successful trial)?</Front>
<Back>Self-controllers have a higher level of OFC activation than non-self-controllers in successful trials</Back>
</Card>

<Card id="26" type="back-front">
<Front>How is value represented in the brain?</Front>
<Back>Orbitofrontal cortex (OFC) represents value and its representation may differ depending on control</Back>
</Card>

<Card id="27" type="back-front">
<Front>What is the Rescorla-Wagner Model?</Front>
<Back>V<sub>t</sub> = (a * (L<sub>t</sub> - V<sub>t-1</sub>)) + V<sub>t-1</sub></Back>
</Card>

<Card id="28" type="back-front">
<Front>V<sub>t</sub></Front>
<Back>Value on trial t</Back>
</Card>

<Card id="29" type="back-front">
<Front>a</Front>
<Back>Learning rate (a constant)</Back>
</Card>

<Card id="30" type="back-front">
<Front>L<sub>t</sub></Front>
<Back>Reward obtained on trial t</Back>
</Card>

<Card id="31" type="back-front">
<Front>V<sub>t-1</sub></Front>
<Back>Value on trial t-1</Back>
</Card>

<Card id="32" type="back-front">
<Front>In the Rescorla-Wagner model, what is value based on?</Front>
<Back>What happened (L<sub>t</sub>) compared to what you expected (V<sub>t-1</sub>) scaled by how much you learn (a) from any given trial</Back>
</Card>

<Card id="33" type="back-front">
<Front>If a = 0...</Front>
<Back>You learn nothing and nothing ever changes</Back>
</Card>

<Card id="34" type="back-front">
<Front>If L<sub>t</sub> = V<sub>t-1</sub>...</Front>
<Back>You learn nothing, the outcome matched the expectation, so value doesn't change</Back>
</Card>

<Card id="35" type="back-front">
<Front>In the Rescorla-Wagner model, what leads to large changes in value?</Front>
<Back>Large differences in expected outcome (V<sub>t-1</sub>) and actual outcome (L<sub>t</sub>)</Back>
</Card>

<Card id="36" type="back-front">
<Front>Prediction error leads to...</Front>
<Back>changes in value</Back>
</Card>

<Card id="37" type="back-front">
<Front>How do we learn about value?</Front>
<Back>Comparing expected to received outcomes; the Rescorla-Wagner model</Back>
</Card>

<Card id="38" type="back-front">
<Front>Is there a signal in the brain that reflects prediction error?</Front>
<Back>Yes</Back>
</Card>

<Card id="39" type="back-front">
<Front>What are the two primary dopamine centers?</Front>
<Back>- Substantia nigra (SN)
- Ventral tegmental area (VTA)</Back>
</Card>

<Card id="40" type="back-front">
<Front>Where does the VTA project to?</Front>
<Back>The nucleus accumbens (NAcc)</Back>
</Card>

<Card id="41" type="cloze">
[[The]] NAcc is part of the ventral striatum
</Card>

<Card id="42" type="back-front">
<Front>What did Olds and Milner's research find?</Front>
<Back>When rats were provided a button that activated an electrode in their VTA, they would self-stimulate at the expense of food, sometimes stimulating until they die.</Back>
</Card>

<Card id="43" type="back-front">
<Front>If dopamine is a reward signal, then...</Front>
<Back>- Any time a reward is present, dopamine should increase (not the case)
- Punishments shouldn't induce dopamine changes (not the case)</Back>
</Card>

<Card id="44" type="back-front">
<Front>In the Schultz et al. study, prior to training, when did the monkey experience a spike in dopamine activation?</Front>
<Back>When they got the juice</Back>
</Card>

<Card id="45" type="back-front">
<Front>In the Schultz et al study, after training, when did the monkey experience a spike in dopamine activation?</Front>
<Back>After the bell (CS) was rung</Back>
</Card>

<Card id="46" type="back-front">
<Front>In the Schultz et al study, after training, what happened if the CS was present, but not followed by the juice?</Front>
<Back>A dip in dopamine activity was observed</Back>
</Card>

<Card id="47" type="back-front">
<Front>How do we know that dopamine doesn't correspond with reward?</Front>
<Back>- When you expect a reward, there is an increase in dopamine, not when you get the reward itself
- When you expect a reward and don't get it, you see a decrease in dopamine</Back>
</Card>

<Card id="48" type="back-front">
<Front>Does dopamine increase whenever someone gets a reward?</Front>
<Back>No. It increases when someone receives an *unexpected* reward, or in anticipation of a reward.</Back>
</Card>

<Card id="49" type="back-front">
<Front>What are the four conditions in Zaghloul et al?</Front>
<Back>- Expected gains
- Expected losses
- Unexpected gains
- Unexpected losses</Back>
</Card>

<Card id="50" type="back-front">
<Front>What is the dependent variable in Zaghloul et al?</Front>
<Back>Firing rate of substantia nigra (SN) neurons</Back>
</Card>

<Card id="51" type="back-front">
<Front>If dopamine represents reward, what can we expect as the outcome for Zaghloul et al?</Front>
<Back>SN firing rate should change for conditions 1 and 3</Back>
</Card>

<Card id="52" type="back-front">
<Front>If dopamine represents prediction errors, what can we expect as the outcome for Zaghloul et al?</Front>
<Back>SN firing rate should change for conditions 3 and 4</Back>
</Card>

<Card id="53" type="back-front">
<Front>What were the results of the Zaghloul et al research for the expected outcomes?</Front>
<Back>Not much change from baseline firing rate</Back>
</Card>

<Card id="54" type="back-front">
<Front>What were the results of the Zaghloul et al research for the unexpected outcomes?</Front>
<Back>Significant deviations from baseline firing rate. Unexpected loss caused a significant dip, while unexpected gain caused a significant jump</Back>
</Card>

<Card id="55" type="back-front">
<Front>How does the work of Zaghloul et all show us that dopamine doesn't equal reward?</Front>
<Back>Dopamine neurons fire less following and unexpected punishment/loss, which means dopamine corresponds more to prediction error</Back>
</Card>

<Card id="56" type="back-front">
<Front>What is the evidence for dopamine being a prediction error signal?</Front>
<Back>- Dopamine does not increase during expected gains/rewards (Schulz et al)
- Dopamine should change during unexpected losses (Zaghloul)</Back>
</Card>

<Card id="57" type="back-front">
<Front>What is the task if Frank et al?</Front>
<Back>The Japanese character task, with an addition</Back>
</Card>

<Card id="58" type="back-front">
<Front>What is the Japanese character task?</Front>
<Back>Participants are shown multiple sets of two Japanese characters. Half the characters represent high reward, half represent low reward. The objective is for the participant to identify and more frequently choose the high reward characters</Back>
</Card>

<Card id="59" type="back-front">
<Front>What were the independent variables in the Frank et al human lesion study?</Front>
<Back>- Reward feedback (positive for choosing positive character, negative for negative character)
- Patient groups (3): on or off L-DOPA, age matched results</Back>
</Card>

<Card id="60" type="back-front">
<Front>What was the hypothesis in Frank et al?</Front>
<Back>Nonmedicated PD patients can't learn from positive feedback, but can learn from negative feedback. Medicated PD patients can learn from positive feedback, but can't learn from negative feedback</Back>
</Card>

<Card id="61" type="back-front">
<Front>What pathway does D1 activate?</Front>
<Back>The direct pathway</Back>
</Card>

<Card id="62" type="back-front">
<Front>What pathway does D2 activate?</Front>
<Back>The indirect pathway</Back>
</Card>

<Card id="63" type="back-front">
<Front>What is the purpose of the additional test phase on Frank et al's modified Japanese character task?</Front>
<Back>To disambiguate positive from negative feedback. If the subject learned from positive feedback, he would be able to choose a character with a value of 80 over a character with a value of 70. If he learned from negative feedback, he would be able to choose a character with a value of 30 over one with a value of 20</Back>
</Card>

<Card id="64" type="back-front">
<Front>What were the results of Frank et al?</Front>
<Back>L-DOPA subjects were better at choosing A over C, but in line with controls at avoiding B, while patients who are L-DOPA deficient choose A with controls and avoid B better than controls</Back>
</Card>

<Card id="65" type="back-front">
<Front>Which subject group showed enhanced "no-go" learning in Frank et al?</Front>
<Back>Unmedicated PD patients</Back>
</Card>

<Card id="66" type="back-front">
<Front>Which group showed enhanced "go" learning in Frank et al?</Front>
<Back>L-DOPA PD patients</Back>
</Card>

<Card id="67" type="back-front">
<Front>In Kravitz et al, what was found regarding stimulating the indirect pathway?</Front>
<Back>Stimulating the indirect, no-go pathway resulted in less frequent pressing of the no-go lever. Reinforce avoidance behavior.</Back>
</Card>

<Card id="68" type="back-front">
<Front>In Kravitz et al, what was found regarding stimulating the direct pathway?</Front>
<Back>It lead to more frequent pressing of the lever. Reinforce the selection behavior.</Back>
</Card>

<Card id="69" type="back-front">
<Front>What does dopamine activity represent or code?</Front>
<Back>Not reward/pleasure, but prediction error: the difference in expected and actual outcomes, both for rewards and punishment</Back>
</Card>

